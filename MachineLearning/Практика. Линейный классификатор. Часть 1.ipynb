{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поймай меня, если сможешь\n",
    "\n",
    "https://www.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2\n",
    "\n",
    "Будем решать задачу идентификации взломщика по его поведению в сети Интернет. Это сложная и интересная задача на стыке анализа данных и поведенческой психологии. В качестве примера, компания Яндекс решает задачу идентификации взломщика почтового ящика по его поведению. В двух словах, взломщик будет себя вести не так, как владелец ящика: он может не удалять сообщения сразу по прочтении, как это делал хозяин, он будет по-другому ставить флажки сообщениям и даже по-своему двигать мышкой. Тогда такого злоумышленника можно идентифицировать и \"выкинуть\" из почтового ящика, предложив хозяину войти по SMS-коду. Этот пилотный проект описан в статье на Хабрахабре. Похожие вещи делаются, например, в Google Analytics и описываются в научных статьях, найти можно многое по фразам \"Traversal Pattern Mining\" и \"Sequential Pattern Mining\".\n",
    "\n",
    "В этом соревновании будем решать похожую задачу: алгоритм будет анализировать последовательность из нескольких веб-сайтов, посещенных подряд одним и тем же человеком, и определять, Элис это или взломщик (кто-то другой).\n",
    "\n",
    "Данные собраны с прокси-серверов Университета Блеза Паскаля. \"A Tool for Classification of Sequential Data\", авторы Giacomo Kahn, Yannick Loiseau и Olivier Raynaud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(\"/Users/iakubovskii/Machine_Learning/RANEPA/Fintech_2020/Машинное обучение/Данные/Alice/\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "            site1               time1  site2               time2  site3  \\\nsession_id                                                                \n21669          56 2013-01-12 08:05:57   55.0 2013-01-12 08:05:57    NaN   \n54843          56 2013-01-12 08:37:23   55.0 2013-01-12 08:37:23   56.0   \n77292         946 2013-01-12 08:50:13  946.0 2013-01-12 08:50:14  951.0   \n114021        945 2013-01-12 08:50:17  948.0 2013-01-12 08:50:17  949.0   \n146670        947 2013-01-12 08:50:20  950.0 2013-01-12 08:50:20  948.0   \n\n                         time3  site4               time4  site5  \\\nsession_id                                                         \n21669                      NaT    NaN                 NaT    NaN   \n54843      2013-01-12 09:07:07   55.0 2013-01-12 09:07:09    NaN   \n77292      2013-01-12 08:50:15  946.0 2013-01-12 08:50:15  946.0   \n114021     2013-01-12 08:50:18  948.0 2013-01-12 08:50:18  945.0   \n146670     2013-01-12 08:50:20  947.0 2013-01-12 08:50:21  950.0   \n\n                         time5  ...               time6  site7  \\\nsession_id                      ...                              \n21669                      NaT  ...                 NaT    NaN   \n54843                      NaT  ...                 NaT    NaN   \n77292      2013-01-12 08:50:16  ... 2013-01-12 08:50:16  948.0   \n114021     2013-01-12 08:50:18  ... 2013-01-12 08:50:18  947.0   \n146670     2013-01-12 08:50:21  ... 2013-01-12 08:50:21  946.0   \n\n                         time7  site8               time8  site9  \\\nsession_id                                                         \n21669                      NaT    NaN                 NaT    NaN   \n54843                      NaT    NaN                 NaT    NaN   \n77292      2013-01-12 08:50:16  784.0 2013-01-12 08:50:16  949.0   \n114021     2013-01-12 08:50:19  945.0 2013-01-12 08:50:19  946.0   \n146670     2013-01-12 08:50:21  951.0 2013-01-12 08:50:22  946.0   \n\n                         time9 site10              time10 target  \nsession_id                                                        \n21669                      NaT    NaN                 NaT      0  \n54843                      NaT    NaN                 NaT      0  \n77292      2013-01-12 08:50:17  946.0 2013-01-12 08:50:17      0  \n114021     2013-01-12 08:50:19  946.0 2013-01-12 08:50:20      0  \n146670     2013-01-12 08:50:22  947.0 2013-01-12 08:50:22      0  \n\n[5 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>site1</th>\n      <th>time1</th>\n      <th>site2</th>\n      <th>time2</th>\n      <th>site3</th>\n      <th>time3</th>\n      <th>site4</th>\n      <th>time4</th>\n      <th>site5</th>\n      <th>time5</th>\n      <th>...</th>\n      <th>time6</th>\n      <th>site7</th>\n      <th>time7</th>\n      <th>site8</th>\n      <th>time8</th>\n      <th>site9</th>\n      <th>time9</th>\n      <th>site10</th>\n      <th>time10</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>session_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>21669</th>\n      <td>56</td>\n      <td>2013-01-12 08:05:57</td>\n      <td>55.0</td>\n      <td>2013-01-12 08:05:57</td>\n      <td>NaN</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>NaT</td>\n      <td>...</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>NaT</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>54843</th>\n      <td>56</td>\n      <td>2013-01-12 08:37:23</td>\n      <td>55.0</td>\n      <td>2013-01-12 08:37:23</td>\n      <td>56.0</td>\n      <td>2013-01-12 09:07:07</td>\n      <td>55.0</td>\n      <td>2013-01-12 09:07:09</td>\n      <td>NaN</td>\n      <td>NaT</td>\n      <td>...</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>NaT</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>77292</th>\n      <td>946</td>\n      <td>2013-01-12 08:50:13</td>\n      <td>946.0</td>\n      <td>2013-01-12 08:50:14</td>\n      <td>951.0</td>\n      <td>2013-01-12 08:50:15</td>\n      <td>946.0</td>\n      <td>2013-01-12 08:50:15</td>\n      <td>946.0</td>\n      <td>2013-01-12 08:50:16</td>\n      <td>...</td>\n      <td>2013-01-12 08:50:16</td>\n      <td>948.0</td>\n      <td>2013-01-12 08:50:16</td>\n      <td>784.0</td>\n      <td>2013-01-12 08:50:16</td>\n      <td>949.0</td>\n      <td>2013-01-12 08:50:17</td>\n      <td>946.0</td>\n      <td>2013-01-12 08:50:17</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>114021</th>\n      <td>945</td>\n      <td>2013-01-12 08:50:17</td>\n      <td>948.0</td>\n      <td>2013-01-12 08:50:17</td>\n      <td>949.0</td>\n      <td>2013-01-12 08:50:18</td>\n      <td>948.0</td>\n      <td>2013-01-12 08:50:18</td>\n      <td>945.0</td>\n      <td>2013-01-12 08:50:18</td>\n      <td>...</td>\n      <td>2013-01-12 08:50:18</td>\n      <td>947.0</td>\n      <td>2013-01-12 08:50:19</td>\n      <td>945.0</td>\n      <td>2013-01-12 08:50:19</td>\n      <td>946.0</td>\n      <td>2013-01-12 08:50:19</td>\n      <td>946.0</td>\n      <td>2013-01-12 08:50:20</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>146670</th>\n      <td>947</td>\n      <td>2013-01-12 08:50:20</td>\n      <td>950.0</td>\n      <td>2013-01-12 08:50:20</td>\n      <td>948.0</td>\n      <td>2013-01-12 08:50:20</td>\n      <td>947.0</td>\n      <td>2013-01-12 08:50:21</td>\n      <td>950.0</td>\n      <td>2013-01-12 08:50:21</td>\n      <td>...</td>\n      <td>2013-01-12 08:50:21</td>\n      <td>946.0</td>\n      <td>2013-01-12 08:50:21</td>\n      <td>951.0</td>\n      <td>2013-01-12 08:50:22</td>\n      <td>946.0</td>\n      <td>2013-01-12 08:50:22</td>\n      <td>947.0</td>\n      <td>2013-01-12 08:50:22</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загрузим обучающую и тестовую выборки\n",
    "train_df = pd.read_csv(\"train_sessions.csv.zip\", index_col=\"session_id\")\n",
    "test_df = pd.read_csv(\"test_sessions.csv.zip\", index_col=\"session_id\")\n",
    "\n",
    "# приведем колонки time1, ..., time10 к временному формату\n",
    "times = [\"time%s\" % i for i in range(1, 11)]\n",
    "train_df[times] = train_df[times].apply(pd.to_datetime)\n",
    "test_df[times] = test_df[times].apply(pd.to_datetime)\n",
    "\n",
    "# отсортируем данные по времени\n",
    "train_df = train_df.sort_values(by=\"time1\")\n",
    "\n",
    "# посмотрим на заголовок обучающей выборки\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обучающей выборке содержатся следующие признаки:\n",
    "\n",
    "- site1 – индекс первого посещенного сайта в сессии\n",
    "- time1 – время посещения первого сайта в сессии\n",
    "- ...\n",
    "- site10 – индекс 10-го посещенного сайта в сессии\n",
    "- time10 – время посещения 10-го сайта в сессии\n",
    "- target – целевая переменная, 1 для сессий Элис, 0 для сессий других пользователей\n",
    "\n",
    "Сессии пользователей выделены таким образом, что они не могут быть длиннее получаса или 10 сайтов. То есть сессия считается оконченной либо когда пользователь посетил 10 сайтов подряд либо когда сессия заняла по времени более 30 минут.\n",
    "\n",
    "В таблице встречаются пропущенные значения, это значит, что сессия состоит менее, чем из 10 сайтов. Заменим пропущенные значения нулями и приведем признаки к целому типу. Также загрузим словарь сайтов и посмотрим, как он выглядит:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "всего сайтов: 48371\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                     site\n25075              www.abmecatronique.com\n13997                     groups.live.com\n42436  majeureliguefootball.wordpress.com\n30911           cdt46.media.tourinsoft.eu\n8104                  www.hdwallpapers.eu",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>site</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>25075</th>\n      <td>www.abmecatronique.com</td>\n    </tr>\n    <tr>\n      <th>13997</th>\n      <td>groups.live.com</td>\n    </tr>\n    <tr>\n      <th>42436</th>\n      <td>majeureliguefootball.wordpress.com</td>\n    </tr>\n    <tr>\n      <th>30911</th>\n      <td>cdt46.media.tourinsoft.eu</td>\n    </tr>\n    <tr>\n      <th>8104</th>\n      <td>www.hdwallpapers.eu</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# приведем колонки site1, ..., site10 к целочисленному формату и заменим пропуски нулями\n",
    "sites = [\"site%s\" % i for i in range(1, 11)]\n",
    "train_df[sites] = train_df[sites].fillna(0).astype(\"int\")\n",
    "test_df[sites] = test_df[sites].fillna(0).astype(\"int\")\n",
    "\n",
    "# загрузим словарик сайтов\n",
    "with open(r\"site_dic.pkl\", \"rb\") as input_file:\n",
    "    site_dict = pickle.load(input_file)\n",
    "\n",
    "# датафрейм словарика сайтов\n",
    "sites_dict_df = pd.DataFrame(\n",
    "    list(site_dict.keys()), index=list(site_dict.values()), columns=[\"site\"]\n",
    ")\n",
    "print(u\"всего сайтов:\", sites_dict_df.shape[0])\n",
    "sites_dict_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим целевую переменную и объединим выборки, чтобы вместе привести их к разреженному формату."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# наша целевая переменная\n",
    "y_train = train_df[\"target\"]\n",
    "\n",
    "# объединенная таблица исходных данных\n",
    "full_df = pd.concat([train_df.drop(\"target\", axis=1), test_df])\n",
    "\n",
    "# индекс, по которому будем отделять обучающую выборку от тестовой\n",
    "idx_split = train_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сессии Алисы из тренировочной выборки : 2297 Сессии не-Алисы из тренировочной выборки : 251264\n"
     ]
    }
   ],
   "source": [
    "print(f\"Сессии Алисы из тренировочной выборки : {(y_train == 1).sum()}\", \n",
    "      f\"Сессии не-Алисы из тренировочной выборки : {(y_train == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для самой первой модели будем использовать только посещенные сайты в сессии (но не будем обращать внимание на временные признаки). За таким выбором данных для модели стоит такая идея: у Элис есть свои излюбленные сайты, и чем чаще вы видим эти сайты в сессии, тем выше вероятность, что это сессия Элис и наоборот.\n",
    "\n",
    "Подготовим данные, из всей таблицы выберем только признаки site1, site2, ... , site10. Напомним, что пропущенные значения заменены нулем. Вот как выглядят первые строки таблицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "            site1  site2  site3  site4  site5  site6  site7  site8  site9  \\\nsession_id                                                                  \n21669          56     55      0      0      0      0      0      0      0   \n54843          56     55     56     55      0      0      0      0      0   \n77292         946    946    951    946    946    945    948    784    949   \n114021        945    948    949    948    945    946    947    945    946   \n146670        947    950    948    947    950    952    946    951    946   \n\n            site10  \nsession_id          \n21669            0  \n54843            0  \n77292          946  \n114021         946  \n146670         947  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>site1</th>\n      <th>site2</th>\n      <th>site3</th>\n      <th>site4</th>\n      <th>site5</th>\n      <th>site6</th>\n      <th>site7</th>\n      <th>site8</th>\n      <th>site9</th>\n      <th>site10</th>\n    </tr>\n    <tr>\n      <th>session_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>21669</th>\n      <td>56</td>\n      <td>55</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>54843</th>\n      <td>56</td>\n      <td>55</td>\n      <td>56</td>\n      <td>55</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>77292</th>\n      <td>946</td>\n      <td>946</td>\n      <td>951</td>\n      <td>946</td>\n      <td>946</td>\n      <td>945</td>\n      <td>948</td>\n      <td>784</td>\n      <td>949</td>\n      <td>946</td>\n    </tr>\n    <tr>\n      <th>114021</th>\n      <td>945</td>\n      <td>948</td>\n      <td>949</td>\n      <td>948</td>\n      <td>945</td>\n      <td>946</td>\n      <td>947</td>\n      <td>945</td>\n      <td>946</td>\n      <td>946</td>\n    </tr>\n    <tr>\n      <th>146670</th>\n      <td>947</td>\n      <td>950</td>\n      <td>948</td>\n      <td>947</td>\n      <td>950</td>\n      <td>952</td>\n      <td>946</td>\n      <td>951</td>\n      <td>946</td>\n      <td>947</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# табличка с индексами посещенных сайтов в сессии\n",
    "full_sites = full_df[sites]\n",
    "full_sites.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сессии представляют собой последовательность индексов сайтов и данные в таком виде неудобны для линейных методов. В соответствии с нашей гипотезой (у Элис есть излюбленные сайты) надо преобразовать эту таблицу таким образом, чтобы каждому возможному сайту соответствовал свой отдельный признак (колонка), а его значение равнялось бы количеству посещений этого сайта в сессии. Это делается примерно вот так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def create_csr(full_sites):\n",
    "    # последовательность с индексами\n",
    "    sites_flatten = full_sites.values.flatten()\n",
    "\n",
    "    # искомая матрица\n",
    "    full_sites_sparse = csr_matrix(\n",
    "        (\n",
    "            [1] * sites_flatten.shape[0],\n",
    "            sites_flatten,\n",
    "            range(0, sites_flatten.shape[0] + 10, 10),\n",
    "        )\n",
    "    )[:, 1:]\n",
    "    return(full_sites_sparse)\n",
    "full_sites_sparse = create_csr(full_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Преимущества формата CSR**\n",
    "  - эффективные арифметические операции CSR + CSR, CSR * CSR и т.д.\n",
    "  - эффективная нарезка строк\n",
    "  - быстрые матрично-векторные произведения\n",
    "\n",
    "**Недостатки формата CSR**\n",
    "  - медленные операции нарезки столбцов (рассмотрим CSC)\n",
    "  - изменение структуры разреженности требует больших затрат (рассмотрите LIL или DOK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<336358x48371 sparse matrix of type '<class 'numpy.int64'>'\n\twith 3195430 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_sites_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168150\n",
      "3195430\n"
     ]
    }
   ],
   "source": [
    "sites_flatten = full_sites.values.flatten()\n",
    "print(np.sum(sites_flatten == 0)) \n",
    "print(np.sum(sites_flatten != 0)) # 3195430 stored elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "48371"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_sites.values.max() # 336358x48371"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Еще один плюс использования разреженных матриц в том, что для них имеются специальные реализации как матричных операций, так и алгоритмов машинного обучения, что подчас позволяет ощутимо ускорить операции за счет особенностей структуры данных. Это касается и логистической регрессии. Вот теперь у нас все готово для построения нашей первой модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Первая модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, у нас есть алгоритм и данные для него, построим нашу первую модель, воспользовавшись релизацией логистической регрессии из пакета sklearn с параметрами по умолчанию. Первые 90% данных будем использовать для обучения (обучающая выборка отсортирована по времени), а оставшиеся 10% для проверки качества (validation).\n",
    "\n",
    "**Напишем простую функцию, которая будет возвращать качество модели на отложенной выборке, и обучим наш первый классификатор**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "min_max_sc = MaxAbsScaler()\n",
    "\n",
    "def get_auc_lr_valid(X, y, C=1.0, ratio=0.9, seed=17):\n",
    "    \"\"\"\n",
    "    X, y – выборка\n",
    "    ratio – в каком отношении поделить выборку\n",
    "    C, seed – коэф-т регуляризации и random_state \n",
    "              логистической регрессии\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=ratio,\n",
    "                                                        random_state=seed)\n",
    "    X_train_sc = min_max_sc.fit_transform(X_train)\n",
    "    X_test_sc = min_max_sc.transform(X_test)\n",
    "    lr = LogisticRegression(C=C)\n",
    "    lr.fit(X_train_sc, y_train)\n",
    "    y_predict_proba = lr.predict_proba(X_test_sc)[:, 1]\n",
    "    return roc_auc_score(y_test, y_predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC на отложенной выборке = 0.9175071185054771\n"
     ]
    }
   ],
   "source": [
    "roc_auc = get_auc_lr_valid(full_sites_sparse[:idx_split], y_train)\n",
    "print(f\"ROC AUC на отложенной выборке = {roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем считать эту модель нашей первой отправной точкой (baseline). Для построения модели для прогноза на тестовой выборке необходимо обучить модель заново уже на всей обучающей выборке (пока наша модель обучалась лишь на части данных), что повысит ее обобщающую способность:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Улучшаем модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим такой признак, который будет представлять собой число вида ГГГГММ от той даты, когда проходила сессия, например 201407 -- 2014 год и 7 месяц. Таким образом, мы будем учитывать помесячный линейный тренд за весь период предоставленных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "full_sites['session_date'] = full_df['time1'].dt.year.apply(str) +  \\\n",
    "                          full_df['time1'].dt.month.apply(str).apply(lambda x: \"0\" + x if len(x)==1 else x)\n",
    "full_sites['session_date'] = full_sites['session_date'].apply(np.int32)\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "full_sites_sparse = hstack([full_sites_sparse, \n",
    "                            ohe.fit_transform(full_sites[['session_date']])])\n",
    "full_sites_sparse = csr_matrix(full_sites_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<336358x48395 sparse matrix of type '<class 'numpy.float64'>'\n\twith 2203256 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_sites_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим новый признак и снова посчитаем ROC AUC на отложенной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC на отложенной выборке = 0.9241290389879272\n"
     ]
    }
   ],
   "source": [
    "roc_auc = get_auc_lr_valid(full_sites_sparse[:idx_split], y_train)\n",
    "print(f\"ROC AUC на отложенной выборке = {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(336358, 48395)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_sites_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим два новых признака: *start_hour* и *morning*.\n",
    "\n",
    "Признак *start_hour* – это час в который началась сессия (от 0 до 23), а бинарный признак *morning* равен 1, если сессия началась утром и 0, если сессия началась позже (будем считать, что утро это если start_hour равен 11 или меньше).\n",
    "\n",
    "Посчитйте ROC AUC на отложенной выборке для выборки с:\n",
    "\n",
    "- сайтами, start_month и start_hour\n",
    "\n",
    "- сайтами, start_month и morning\n",
    "\n",
    "- сайтами, start_month, start_hour и morning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sites['start_hour'] = full_df['time1'].dt.hour\n",
    "full_sites['start_month'] = full_df['time1'].dt.month\n",
    "full_sites['mornning'] = full_sites['start_hour'].apply(lambda x: 1 if x <=11 else 0) \n",
    "\n",
    "from scipy.sparse import hstack\n",
    "full_sites_sparse = hstack([full_sites_sparse, \n",
    "                            ohe.fit_transform(full_sites[['start_hour']]),\n",
    "                            ohe.fit_transform(full_sites[['start_month']]),\n",
    "                           ohe.fit_transform(full_sites[['mornning']])])\n",
    "full_sites_sparse = csr_matrix(full_sites_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC на отложенной выборке = 0.9659527414002123\n"
     ]
    }
   ],
   "source": [
    "roc_auc = get_auc_lr_valid(full_sites_sparse[:idx_split], y_train)\n",
    "print(f\"ROC AUC на отложенной выборке = {roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбор коэффицициента регуляризации\n",
    "\n",
    "Итак, мы ввели признаки, которые улучшают качество нашей модели по сравнению с первым бейзлайном. Можем ли мы добиться большего значения метрики? После того, как мы сформировали обучающую и тестовую выборки, почти всегда имеет смысл подобрать оптимальные гиперпараметры -- характеристики модели, которые не изменяются во время обучения. В используемой нами логистической регрессии веса каждого признака изменяются и во время обучения находится их оптимальные значения, а коэффициент регуляризации остается постоянным. Это тот гиперпараметр, который мы сейчас будем оптимизировать.\n",
    "\n",
    "Посчитаем качество на отложенной выборке с коэффициентом регуляризации, который по умолчанию C=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь погнали искать оптимальный гиперпараметр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.linspace(0.01, 10, 10)\n",
    "results_C = dict(zip(C, list(map(lambda x: get_auc_lr_valid(full_sites_sparse[:idx_split], y_train, C=x),\n",
    "                            C\n",
    "                           )\n",
    "                            )\n",
    "                    )\n",
    "                )\n",
    "# roc_auc = get_auc_lr_valid(full_sites_sparse[:idx_split], y_train)\n",
    "# print(f\"ROC AUC на отложенной выборке = {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{0.01: 0.905024088946266,\n 1.12: 0.9667908058755479,\n 2.23: 0.9703711453023449,\n 3.34: 0.9714733688677837,\n 4.45: 0.9718817969551969,\n 5.5600000000000005: 0.9719913282937988,\n 6.67: 0.9720029726627656,\n 7.78: 0.9719541562192732,\n 8.89: 0.9718823311802558,\n 10.0: 0.97178133875601}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Использование LASSO-регрессии для выкидывания ненужных признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Для того, чтобы повысить точность, делайте, что хотите \n",
    "\n",
    "https://www.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейные модели, которые мы обсуждали на предыдущем занятии, предполагают в качестве зависимой переменной *количественный* признак. Однако, зачастую, наш целевой признак имеет конечное количество значений и является *качественным*. Например, признак *цвет глаз* имеет всего 3 основных значения: голубой, зеленый, карий. В этом случае мы не можем применять линейные модели и говорим, что мы решаем задачу классификации, т.е. классифицируем каждое наблюдение соответствующему значению целевой переменной.\n",
    "\n",
    "Отметим, что существуют два различных подхода к решению задачи классификации. Первый подход включает в себя методы, которые позволяют сразу относить наблюдение к тому или иному признаку (например, метод ближайших соседей). Второй подход вклюает в себя методы, которые сначала вычисляют вероятность отнесения к какому-либо классу, а потом уже задавая различные пороги отнесения к тому или иному классу, мы принимаем решение к принадлежности класса (например, логистическая регрессия).\n",
    "\n",
    "Задача классификации встречается на практике обычно даже чаще, чем задача регрессии. Приведем пример некоторых задач:\n",
    "\n",
    "- Выявление дефолтных заемщиков в банке (банкрот или не банкрот)\n",
    "\n",
    "- Выявление людей с различными заболеваниями (есть заболевание или нет)\n",
    "\n",
    "- Мошенническая транзакция или нет \n",
    "\n",
    "и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***В этой лекции прошу обратить внимание на методы kNN и логистическую регрессию. Все остальные методы изучайте чисто для вашего развития, если есть свободное время.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Метод ближайших соседей (kNN)\n",
    "\n",
    "Метод ближайших соседей является одним из самых простых и базовых алгоритмов машинного обучения. \"Соседями\" в этом методе называются те наблюдения из тестовой выборки, расстояния от которых наименьшее к наблюдениям из тренировочной выборки. Соответственно, задача данного метода заключается в том, чтобы каждому значению целевой переменной из тестовой выборки сопоставить то значение, которое наиболее часто встречается у \"соседей\" данного наблюдения в тренировочной выборке. Количество соседей здесь - это количество наблюдений из тренировочной выборки, которые наиболее близки к наблюдениям из тестовой выборки.\n",
    "\n",
    "Расстояние от одного наблюдения к другому может быть измерено несколькими способами (обязательна стандартизация, иначе расстояние будет рассчитываться некорректно):\n",
    "\n",
    "**Эвклидово расстояние** между точками p = ($p_1$, $p_2$,..., $p_n$) и q = ($q_1$, $q_2$,..., $q_n$):\n",
    "\n",
    "$$ d(p, q) = d(q, p) = \\sqrt{(q_1 - p_1)^2 + (q_2 - p_2)^2 + ... + (q_n - p_n)^2} $$\n",
    "\n",
    "**Расстояние Миньковского** порядка $p$ (для p=2 - эвклидово расстояние):\n",
    "\n",
    "X= $(x_{1},x_{2},\\ldots ,x_{n})$ and Y = $(y_{1},y_{2},\\ldots ,y_{n}) \\in {R}^{n}$:\n",
    "\n",
    "$$ D\\left(X,Y\\right)=\\left(\\sum _{i=1}^{n}|x_{i}-y_{i}|^{p}\\right)^{\\frac {1}{p}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем применить данный метод при помощи различных расстояний и количества соседей на таком игрушечном наборе данных:\n",
    "\n",
    "|Obs. | X1 | X2 | X3 | Y | \n",
    "| --  |-- | -- | -- | -- | \n",
    "|1 | 0 |3 |0 | Red |\n",
    "|2 | 2 | 0 | 0| Red |\n",
    "|3 | 0| 1| 3 | Red | \n",
    "|4 | 0| 1| 2| Green | \n",
    "|5 |−1 |0 |1 |Green \n",
    "|6 | 1| 1| 1| Red\n",
    "\n",
    "Наша задача предсказать цвет шарика (Y) для точки (0, 0, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.         2.         3.03658897 2.08008382 1.25992105 1.44224957]\n",
      "[3.         2.         3.03658897 2.08008382 1.25992105 1.44224957]\n",
      "[3.         2.         3.16227766 2.23606798 1.41421356 1.73205081]\n",
      "[3.         2.         3.16227766 2.23606798 1.41421356 1.73205081]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "y_dict = {\"Red\": 1, \"Green\": 0}\n",
    "X_train = np.array([[0, 3, 0], [2, 0, 0], [0, 1, 3], [0, 1, 2], [-1, 0, 1], [1,1,1]])\n",
    "y_train = np.array(['Red', 'Red', 'Red', \"Green\", \"Green\", \"Red\"])\n",
    "X_test = np.array([0, 0, 0])\n",
    "\n",
    "def minkowsi_dist(v1, v2, p=3):\n",
    "    \"\"\"v1, v2: вектора\"\"\"\n",
    "    sum_ = sum(map(lambda v1, v2: np.abs(v1 - v2)**p, v1, v2))\n",
    "    return np.sign(sum_) * np.abs(sum_) ** (1/p)\n",
    "\n",
    "# Проверка при помощи готовых реализаций numpy, scipy\n",
    "dist_mat = np.apply_along_axis(minkowsi_dist, 1, X_train, X_test)\n",
    "dist_mat_eucl = np.apply_along_axis(minkowsi_dist, 1, X_train, X_test, p=2)\n",
    "from scipy.spatial import distance\n",
    "dist_mat_1 = np.apply_along_axis(distance.minkowski, 1, X_train, X_test, p=3)\n",
    "dist_mat_2 = np.apply_along_axis(distance.minkowski, 1, X_train, X_test)\n",
    "\n",
    "print(dist_mat)\n",
    "print(dist_mat_1)\n",
    "print(dist_mat_eucl)\n",
    "print(dist_mat_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Red'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = dist_mat_1\n",
    "k = 3\n",
    "smallest_dist_n = np.argsort(dm)[0:k]\n",
    "max(y_train[smallest_dist_n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0}\n",
      "{1: 0.0, 2: 0.0, 3: 1.0, 4: 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Переводим цвет шариков в численный тип\n",
    "y_train_1_0 = pd.Series(y_train).map(y_dict)\n",
    "y_test_1_0 = pd.Series(y_test).map(y_dict)\n",
    "\n",
    "# Предскажем цвет шарика при помощи разного количества соседей для эвклидового расстояния.\n",
    "def predict_knn(y_train, dm, k):\n",
    "    smallest_dist_n = np.argsort(dm)[0:k]\n",
    "    return np.round(np.mean(y_train[smallest_dist_n]))\n",
    "neibors_ = np.arange(1,5)\n",
    "predicted_knn_mink = dict(zip(neibors_, \n",
    "                           list(map(lambda x: predict_knn(y_train_1_0, dist_mat_1, k=x), \n",
    "                           neibors_)))\n",
    "                      )\n",
    "print(predicted_knn_mink)\n",
    "predicted_knn_eucl = dict(zip(neibors_, \n",
    "                           list(map(lambda x: predict_knn(y_train_1_0, dist_mat_eucl, k=x), \n",
    "                           neibors_)))\n",
    "                      )\n",
    "print(predicted_knn_eucl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array(['Green'], dtype='<U5'),\n",
       " 2: array(['Green'], dtype='<U5'),\n",
       " 3: array(['Red'], dtype='<U5'),\n",
       " 4: array(['Green'], dtype='<U5')}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Реализация из sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def sklearn_knn(X_train, y_train, X_test, k):\n",
    "    knn_skl = KNeighborsClassifier(n_neighbors=k, p=3)\n",
    "    knn_fit = knn_skl.fit(X_train, y_train)\n",
    "    knn_predict = knn_fit.predict(X_test.reshape(1,-1))\n",
    "    return(knn_predict)\n",
    "predicted_knn_sklearn = dict(zip(neibors_, \n",
    "                           list(map(lambda x: sklearn_knn(\n",
    "                               X_train, y_train, X_test, k=x), \n",
    "                           neibors_)))\n",
    "                      )\n",
    "predicted_knn_sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем применить ближайших соседей на реальных статистических данных.\n",
    "\n",
    "Caravan (страховка для трейлеров) датасет включает в себя 85 предиктора, измеряющих различные социально-демографические характеристики для 5822 респондентов. Целевая переменная - *Purchase*, совершил ли индивид покупку страховки Caravan. Отметим, что только 6% людей купило страховку caravan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MOSTYPE</th>\n",
       "      <th>MAANTHUI</th>\n",
       "      <th>MGEMOMV</th>\n",
       "      <th>MGEMLEEF</th>\n",
       "      <th>MOSHOOFD</th>\n",
       "      <th>MGODRK</th>\n",
       "      <th>MGODPR</th>\n",
       "      <th>MGODOV</th>\n",
       "      <th>MGODGE</th>\n",
       "      <th>MRELGE</th>\n",
       "      <th>...</th>\n",
       "      <th>APERSONG</th>\n",
       "      <th>AGEZONG</th>\n",
       "      <th>AWAOREG</th>\n",
       "      <th>ABRAND</th>\n",
       "      <th>AZEILPL</th>\n",
       "      <th>APLEZIER</th>\n",
       "      <th>AFIETS</th>\n",
       "      <th>AINBOED</th>\n",
       "      <th>ABYSTAND</th>\n",
       "      <th>Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MOSTYPE  MAANTHUI  MGEMOMV  MGEMLEEF  MOSHOOFD  MGODRK  MGODPR  MGODOV  \\\n",
       "0       33         1        3         2         8       0       5       1   \n",
       "1       37         1        2         2         8       1       4       1   \n",
       "2       37         1        2         2         8       0       4       2   \n",
       "3        9         1        3         3         3       2       3       2   \n",
       "4       40         1        4         2        10       1       4       1   \n",
       "\n",
       "   MGODGE  MRELGE  ...  APERSONG  AGEZONG  AWAOREG  ABRAND  AZEILPL  APLEZIER  \\\n",
       "0       3       7  ...         0        0        0       1        0         0   \n",
       "1       4       6  ...         0        0        0       1        0         0   \n",
       "2       4       3  ...         0        0        0       1        0         0   \n",
       "3       4       5  ...         0        0        0       1        0         0   \n",
       "4       4       7  ...         0        0        0       1        0         0   \n",
       "\n",
       "   AFIETS  AINBOED  ABYSTAND  Purchase  \n",
       "0       0        0         0        No  \n",
       "1       0        0         0        No  \n",
       "2       0        0         0        No  \n",
       "3       0        0         0        No  \n",
       "4       0        0         0        No  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "os.chdir(\"/Users/iakubovskii/Machine_Learning/RANEPA/Fintech_2020/Машинное обучение/Данные\")\n",
    "glob.glob(\"*.csv\")\n",
    "caravan = pd.read_csv(\"Caravan.csv\")\n",
    "caravan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим выборку на тренировочную (70%) и тестовую (30%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(caravan.drop(\"Purchase\", axis=1),\n",
    "                                                   caravan['Purchase'], test_size=0.3,\n",
    "                                                   random_state = 42 # тип рандома\n",
    "                                                   )\n",
    "# Препроцессинг - нормирование признаков\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc_x_train = sc.fit(X_train)\n",
    "X_train_scal = sc_x_train.transform(X_train)\n",
    "X_test_scal = sc_x_train.transform(X_test)\n",
    "\n",
    "def sklearn_caravan_knn(X_train, y_train, X_test, k):\n",
    "    knn_skl = KNeighborsClassifier(n_neighbors=k, p=3)\n",
    "    knn_fit = knn_skl.fit(X_train, y_train)\n",
    "    knn_predict = knn_fit.predict(X_test)\n",
    "    return(knn_predict)\n",
    "\n",
    "caravan_predict = sklearn_caravan_knn(X_train_scal, y_train.values, X_test_scal, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'No': 1718, 'Yes': 29})\n",
      "0.9232970807097882\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(caravan_predict))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, caravan_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: Counter({'No': 1644, 'Yes': 103}),\n",
       " 2: Counter({'No': 1737, 'Yes': 10}),\n",
       " 3: Counter({'No': 1718, 'Yes': 29}),\n",
       " 4: Counter({'No': 1741, 'Yes': 6})}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caravan_precict_sklearn = dict(zip(neibors_, \n",
    "                           list(map(lambda x: Counter(sklearn_caravan_knn(\n",
    "                               X_train_scal, y_train, X_test_scal, k=x)\n",
    "                                                     ), \n",
    "                           neibors_)\n",
    "                               )\n",
    "                                  )\n",
    "                              )\n",
    "caravan_precict_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: Counter({'No': 1642, 'Ye': 105}),\n",
       " 2: Counter({'No': 1642, 'Ye': 105}),\n",
       " 3: Counter({'No': 1721, 'Ye': 26}),\n",
       " 4: Counter({'No': 1725, 'Ye': 22})}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def knn_manually(X_train, y_train, X_test, k):\n",
    "    dist_mat_eucl = np.apply_along_axis(minkowsi_dist, 0, X_train.T, X_test.T, p=2)\n",
    "    smallest_dist_n = dist_mat_eucl.argpartition(kth = k, axis = 1)[:, 0:k]\n",
    "    \n",
    "    return np.apply_along_axis(lambda x: Counter(y_train.iloc[x]).most_common()[0][0], \n",
    "                               1, smallest_dist_n)\n",
    "caravan_precict_manually = dict(zip(neibors_, \n",
    "                           list(map(lambda x: Counter(knn_manually(\n",
    "                               X_train_scal, y_train, X_test_scal, k=x)\n",
    "                                                     ), \n",
    "                           neibors_)\n",
    "                               )\n",
    "                                  )\n",
    "                              )\n",
    "caravan_precict_manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: Counter({0: 1642, 1: 105}),\n",
       " 2: Counter({0: 1642, 1: 105}),\n",
       " 3: Counter({0: 1721, 1: 26}),\n",
       " 4: Counter({0: 1725, 1: 22})}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_1_0, y_test_1_0 = y_train.map({\"Yes\": 1, \"No\":0}), y_test.map({\"Yes\": 1, \"No\":0})\n",
    "caravan_precict_manually = dict(zip(neibors_, \n",
    "                           list(map(lambda x: Counter(knn_manually(\n",
    "                               X_train_scal, y_train_1_0, X_test_scal, k=x)\n",
    "                                                     ), \n",
    "                           neibors_)\n",
    "                               )\n",
    "                                  )\n",
    "                              )\n",
    "caravan_precict_manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_manually = {k: knn_manually(X_train_scal, y_train_1_0, X_test_scal, k=k) for k in range(1,10)}\n",
    "y_pred_sklearn = {k: sklearn_caravan_knn(X_train_scal, y_train_1_0, X_test_scal, k=k) for k in range(1,10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Разница в прогнозах для 1 соседей = 30\n",
      " Разница в прогнозах для 2 соседей = 97\n",
      " Разница в прогнозах для 3 соседей = 19\n",
      " Разница в прогнозах для 4 соседей = 20\n",
      " Разница в прогнозах для 5 соседей = 4\n",
      " Разница в прогнозах для 6 соседей = 8\n",
      " Разница в прогнозах для 7 соседей = 4\n",
      " Разница в прогнозах для 8 соседей = 3\n",
      " Разница в прогнозах для 9 соседей = 0\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 10):\n",
    "    print(f\" Разница в прогнозах для {k} соседей = {sum(y_pred_manually[k] != y_pred_sklearn[k])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5514c655f8e045268c1d69c6ecd51d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Чтобы не тратить время на копирование и вставку почти одних и тех же переменных, можно\n",
    "# воспользоваться командой globals()\n",
    "from tqdm.notebook import tqdm\n",
    "for k in tqdm(range(1, 3)):\n",
    "    globals()[\"y_pred_manually_knn_\" + str(k)] = knn_manually(\n",
    "        X_train_scal, y_train_1_0, X_test_scal, k=k)\n",
    "    globals()[\"y_pred_sklearn_knn_\" + str(k)] = sklearn_caravan_knn(\n",
    "        X_train_scal, y_train_1_0, X_test_scal, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNeighborsClassifier??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Достоинства и недостатки метода ближайших соседей***\n",
    "\n",
    "**Достоинства**:\n",
    "\n",
    "- Простая реализация;\n",
    "- Хорошо изучена;\n",
    "- Как правило, метод является хорошим базовым решением не только для классификации или регрессии, но и для рекомендаций;\n",
    "- Он может быть адаптирован к любой задаче путем выбора правильной метрики. Кстати, [Александр Дьяконов](https://www.kaggle.com/dyakonov), бывший каглер top-1, любит kNN, но с настраиваемой метрикой сходства объектов;\n",
    "- Хорошая интерпретируемость. Есть исключения: при большом количестве соседей интерпретируемость ухудшается (\"Мы ему не давали взаймы, потому что он похож на 350 клиентов, из которых 70 - плохие, а это на 12% больше, чем в среднем по набору данных\").\n",
    "\n",
    "**Недостатки**:\n",
    "\n",
    "- Метод считается довольно быстрым, но на практике количество соседей, используемых для классификации, обычно велико (100-150), и в этом случае алгоритм будет работать не так быстро.\n",
    "- Если в наборе данных много переменных, то трудно найти нужные веса и определить, какие из них не важны для классификации/регрессии.\n",
    "- Зависимость от выбранной метрики расстояния между объектами. Выбор евклидового расстояния по умолчанию часто является необоснованным. Хорошее решение можно найти с помощью поиска по параметрам кросс-валидации, но это становится очень трудоемким для больших наборов данных.\n",
    "- Теоретических способов выбора количества соседей нет - только поиск по сетке (хотя часто это верно для всех гиперпараметров всех моделей). \n",
    "- Как правило, он работает не очень хорошо, когда в модели много признаков из-за \"проклятия размерности\". Профессор Педро Домингос, известный член сообщества ML, рассказывает об этом [здесь](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) в своей популярной работе \"Немного полезного о машинном обучении\"; также \"проклятие размерности\" описано в книге \"Глубокое изучение\" в [этой главе](http://www.deeplearningbook.org/contents/ml.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Дискриминативные модели\n",
    "\n",
    "Традиционно в машинном обучении выделяют два вида моделей, решающих задачу классификации.\n",
    "\n",
    "Пусть $X$ - признаковое пространство, $y$ - вектор целевой переменной. Тогда вероятностное пространство значений целевой переменной $y$ при заданных значениях признаков $p(x,y) = P(y)p(x|y)$. Читается это так: совместная плотность распределения $x,y$ равняется произведению априорных вероятностей классов к функции правдоподобия классов. \n",
    "\n",
    "1. Дискриминативная – моделирует условное распределение $p(x|y)$.\n",
    "\n",
    "2. Генеративная – моделирует полное распределение $p(x,y)$.\n",
    "\n",
    "Генеративные модели более общие: если известно $p(x,y)$, то, используя определение\n",
    "условной вероятности, можно найти $p(x|y)$:\n",
    "\n",
    "$$ p(x|y) = \\frac{p(x,y)}{P(y)} $$\n",
    "\n",
    "До недавнего времени при решении задач классификации или регрессии обычно использовали дискриминативные модели. Основная причина этого, сформулированная в работе Vapnik, V. N., 1998, заключается в том, что необходимо решать задачу (классификации или регрессии) напрямую, а не решать более общую проблему как промежуточный шаг (например,\n",
    "моделирование $p(x|y)$ из $p(x, y)$). Но уже в 2001 году было показано, что при использовании дискриминативной модели не во всех задачах достигается меньшая ошибка, чем при использовании генеративной модели. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Логистическая регрессия\n",
    "\n",
    "Логистическая регрессия - один из наиболее известных видов дискриминативных моделей и является частным случаем линейного классификатора. Основная идея линейного классификатора состоит в том, что два целевых класса могут быть разделены гиперплоскостью в признаковом пространстве. Если мы можем это сделать без ошибок, тренировочная выборка называется *линейно разделимой*. \n",
    "![](https://i1.wp.com/appliedmachinelearning.blog/wp-content/uploads/2017/03/svm_logo1.png?resize=380%2C290&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже знакомы с линейной регрессией. Давайте попробуем использовать наши знания для решения задачи классификации. Предположим, что наш целевой признак принимает два значения: -1 и +1. Один из наиболее простых способов получить результат классификации, используя линейную регрессию - это применить функцию сигнум:\n",
    "\n",
    "$$\\Large a(\\textbf{x}) = \\text{sign}(\\textbf{w}^\\text{T}\\textbf x),$$\n",
    "\n",
    "где\n",
    " - $\\textbf{x}$ –  признаковое пространство;\n",
    " - $\\textbf{w}$ – вектор с весами в линейной модели ($w_0$ - константа);\n",
    " - $\\text{sign}(\\bullet)$ – функция, которая возвращает знак аргумента;\n",
    " - $a(\\textbf{x})$ – отклик классификации при заданных $\\textbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия - это особый случай линейного классификатора с некоторым преимуществом, которое заключается в том, что мы можем предсказать не только сам класс, но и его вероятность:\n",
    "\n",
    "$$\\Large p_+ = P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) $$\n",
    "\n",
    "Такой подход становится важным, когда мы решаем многие бизнес-задачи: от кредитного скоринга до определения спама. Например, мы выбирает порог $𝑝*$, чтобы предсказать вероятность дефолта по кредиту. Более того, можно умножить эту прогнозируемую вероятность на сумму кредита, чтобы получить ожидаемые убытки от клиента, что также может являться хорошими бизнес-показателями (эксперты по скорингу могут добавить больше, но основная суть в этом). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы предсказать вероятность $p_+ \\in [0,1]$ мы можем построить линейные предсказания при помощи МНК: $b(\\textbf{x}) = \\textbf{w}^\\text{T} \\textbf{x} \\in \\mathbb{R}$. Затем мы сопоставляем каждому значению вероятность на отрезке [0,1], используя выбранную функцию: $f: \\mathbb{R} \\rightarrow [0,1]$. В логистической регрессии такая функция называется сигмоида:\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$$\n",
    "\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSmTY7ch1O6eaIm_3CuzkhWgeU8SIUoxOPw9w&usqp=CAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что вероятность наступления события $X$ = $P(X)$. Вспомним, что шанс наступления события - это отношение вероятности к обратной вероятности данного события: $OR(X)_p+ = \\frac{P(X)}{1-P(X)}$. По сути, шанс несет в себе абсолютно ту же информацию, что и вероятность. Однако, если вероятность определена на отрезке [0,1], шанс определен на интервале $(0;\\infty)$. Например, когда мы говорим, что вероятность выиграть 100 рублей в лотерею равняется 0.01, это означает, что шанс выиграть равен $\\frac{0.01}{1-0.01} = \\frac{1}{99}$ (в простонародье 99 к 1). \n",
    "\n",
    "Если мы возьмем логарифм шанса (логарифм правдоподобия), то  $\\log{OR(X)} \\in \\mathbb{R}$. Теперь посмотрим, как, при помощи логистической регрессии, мы прогнозируем вероятности отнесения к тому или иному классу  $p_+ = P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)$:\n",
    "\n",
    "1. Вычисляем веса $w_{0}+w_{1}x_1 + w_{2}x_2 + ... = \\textbf{w}^\\text{T}\\textbf{x}$. Уравнение $\\textbf{w}^\\text{T}\\textbf{x} = 0$ определяет гиперплоскость, которая разделяет выборку на два класса;\n",
    "\n",
    "2. Вычисляем логарифм шансов: $ \\log(OR_{+}) = \\textbf{w}^\\text{T}\\textbf{x}$\n",
    "\n",
    "3. Теперь из шансов переходим к вероятностям классов: \n",
    "\n",
    "$$\\large p_{+} = \\frac{OR_{+}}{1 + OR_{+}} = \\frac{\\exp^{\\textbf{w}^\\text{T}\\textbf{x}}}{1 + \\exp^{\\textbf{w}^\\text{T}\\textbf{x}}} = \\frac{1}{1 + \\exp^{-\\textbf{w}^\\text{T}\\textbf{x}}} = \\sigma(\\textbf{w}^\\text{T}\\textbf{x})$$\n",
    "\n",
    "Видим, что справа у нас стоит сигмоидная функция. Таким образом, логистическая регрессия предсказывает вероятность отнесения к положительному классу при помощи сигмоидной трансформации линейной комбинации весов и признаков:\n",
    "\n",
    "$$\\large p_+(\\textbf{x}_\\text{i}) = P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}). $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим, как вычисляются оценки весов в логистической регрессии. \n",
    "\n",
    "Пусть вероятность отнесения к положительному классу:\n",
    "$$\\Large p_+(\\textbf{x}_\\text{i}) = P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^T\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Пусть вероятность отнесения к отрицательному классу:\n",
    "$$\\Large p_-(\\textbf{x}_\\text{i})  = P\\left(y_i = -1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)  = 1 - \\sigma(\\textbf{w}^T\\textbf{x}_\\text{i}) = \\sigma(-\\textbf{w}^T\\textbf{x}_\\text{i}) $$\n",
    "\n",
    "Объединяя данные два уравнения, получим:\n",
    "\n",
    "$$\\Large P\\left(y = y_i(-1;1) \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(y_i\\textbf{w}^T\\textbf{x}_\\text{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выражение $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^T\\textbf{x}_\\text{i}$ называется отсупом классификации для объекта $\\textbf{x}_\\text{i}$. Если он неотрицательный, то модель корректно предсказывает класс. Если отрицательный, то неверно. Отметим, что отступ определен только для объектов тренировочной выборки, где метка класса известна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь вычислим правдоподобие нашего датасета, т.е. вероятность наблюдать вектор $\\textbf{y}$ в признаковом пространстве $X$ в предположении, что объекты независимо одинаково распределены. Тогда:\n",
    "\n",
    "$$\\Large P\\left(\\textbf{y} \\mid \\textbf{X}, \\textbf{w}\\right) = \\prod_{i=1}^{\\ell} P\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right),$$\n",
    "\n",
    "где $\\ell$ количество наблюдений в матрице $\\textbf{X}$.\n",
    "Что такое правдоподобие классно объясняется в [этом видео](https://www.youtube.com/watch?v=pYxNSUDSFH4&t=241s). (осторожно, он иногда поет....)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее возьмем логарифм, чтобы преобразовать это в сумму:\n",
    "\n",
    "$$\\Large \\log P\\left(\\textbf{y} \\mid \\textbf{X}, \\textbf{w}\\right) = \\log \\prod_{i=1}^{\\ell} P\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\log \\prod_{i=1}^{\\ell} \\sigma(y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})   = $$\n",
    "\n",
    "$$\\Large  = \\sum_{i=1}^{\\ell} \\log \\sigma(y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) = \\sum_{i=1}^{\\ell} \\log \\frac{1}{1 + \\exp^{-y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}}} = - \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Максимизация суммы логарифмов с обратным знаком идентична минимизации:\n",
    "\n",
    "$$\\Large \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}}).$$\n",
    "\n",
    "Это *логистическая* функция потерь, которая суммируется по всем объектам тренировочной выборки. \n",
    "\n",
    "Для сравнения функция потерь в случае линейной регрессии выглядит вот так:\n",
    "\n",
    "$$\\Large \\mathcal{L} (\\textbf X, \\textbf{y}, \\textbf{w}) = (\\textbf{y} - \\textbf{w}\\textbf{X})^T (\\textbf{y} - \\textbf{w}\\textbf{X}) = \n",
    "\\sum_{i=1}^{\\ell} (\\textbf{y}_i - \\textbf{w}\\textbf{X}_i) (\\textbf{y}_i - \\textbf{w}\\textbf{X}_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Регуляризация для логистической регрессии**\n",
    "\n",
    "Вместо минимизации функции $\\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w})$ мы минимизируем следующую функцию потерь:\n",
    "\n",
    "$$\\Large \\mathcal{J}(\\textbf X, \\textbf{y}, \\textbf{w}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}) + \\lambda |\\textbf{w}|^2$$\n",
    "\n",
    "В случае логистической регрессии, коэффициент обратной регуляризации обычно используют $C = \\frac{1}{\\lambda}$. Тогда решения нашей задачи имеет вид:\n",
    "\n",
    "$$\\Large \\widehat{\\textbf w}  = \\arg \\min_{\\textbf{w}} \\mathcal{J}(\\textbf X, \\textbf{y}, \\textbf{w}) =  \\arg \\min_{\\textbf{w}}\\ (C\\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}})+ |\\textbf{w}|^2)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Темы, изложенные ниже, вы можете изучить самостоятельно.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2  Метод опорных векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Перцептрон"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Генеративные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Наивный Байесовский классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Линейный дискриминантный анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Квадратичный дискриминантный анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cлучайный лес в реальной задаче\n",
    "\n",
    "Для этого будем использовать пример с задачей fraud detection. Это задача классификации, поэтому будем использовать метрику accuracy для оценки точности. Для начала построим самый простой классификатор, который будет нашим бейслайном. Возьмем только числовые признаки для упрощения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy score: 92.50%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "# Загружаем данные\n",
    "df = pd.read_csv(\"/Users/iakubovskii/Machine_Learning/RANEPA/Fintech_2020/Анализ данных/Данные/telecom_churn.csv\")\n",
    "\n",
    "# Выбираем сначала только колонки с числовым типом данных\n",
    "cols = []\n",
    "[cols.append(i) for i in df.columns if (df[i].dtype == \"float64\") or (df[i].dtype == 'int64')]\n",
    "        \n",
    "# Разделяем на признаки и объекты\n",
    "X, y = df[cols], df[\"Churn\"].values\n",
    "\n",
    "# Инициализируем стратифицированную разбивку нашего датасета для валидации\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Инициализируем наш классификатор с дефолтными параметрами\n",
    "rfc = RandomForestClassifier(random_state=42, n_jobs=-1, oob_score=True)\n",
    "\n",
    "# Обучаем на тренировочном датасете\n",
    "results = cross_val_score(rfc, X, y, cv=skf)\n",
    "\n",
    "# Оцениваем точность на тестовом датасете\n",
    "print(\"CV accuracy score: {:.2f}%\".format(results.mean()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили точность 92.5%, теперь попробуем улучшить этот результат и посмотреть, как ведут себя кривые обучения при изменении\n",
    "основных параметров.\n",
    "\n",
    "Начнем с количества деревьев:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Инициализируем валидацию\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Создаем списки для сохранения точности на тренировочном и тестовом датасете\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "temp_train_acc = []\n",
    "temp_test_acc = []\n",
    "trees_grid = [100, 150, 200, 250]\n",
    "\n",
    "# Обучаем на тренировочном датасете\n",
    "for ntrees in trees_grid:\n",
    "    rfc = RandomForestClassifier(n_estimators=ntrees, random_state=42, n_jobs=-1, oob_score=True)\n",
    "    temp_train_acc = []\n",
    "    temp_test_acc = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        rfc.fit(X_train, y_train)\n",
    "        temp_train_acc.append(rfc.score(X_train, y_train))\n",
    "        temp_test_acc.append(rfc.score(X_test, y_test))\n",
    "    train_acc.append(temp_train_acc)\n",
    "    test_acc.append(temp_test_acc)\n",
    "    \n",
    "train_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\n",
    "print(\"Best accuracy on CV is {:.2f}% with {} trees\".format(max(test_acc.mean(axis=1))*100, \n",
    "                                                        trees_grid[np.argmax(test_acc.mean(axis=1))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(trees_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\n",
    "ax.plot(trees_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='cv')\n",
    "ax.fill_between(trees_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\n",
    "ax.fill_between(trees_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylim([0.88,1.02])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"N_estimators\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На рисунке видно, что при увеличении количества деревьев, наша точность модели на кросс-валидации немного снижается. Также видим, что на тренировочной выборке мы достигаем 100% точности, это говорит нам о переобучении нашей модели. Чтобы избежать переобучения, мы должны добавить параметры регуляризации в модель.\n",
    "\n",
    "Начнем с параметра максимальной глубины – `max_depth`. (зафиксируем к-во деревьев 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем списки для сохранения точности на тренировочном и тестовом датасете\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "temp_train_acc = []\n",
    "temp_test_acc = []\n",
    "max_depth_grid = [3, 5, 7, 9, 11, 13, 15, 17, 20, 22, 24]\n",
    "\n",
    "# Обучаем на тренировочном датасете\n",
    "for max_depth in max_depth_grid:\n",
    "    rfc = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, oob_score=True, \n",
    "                                 max_depth=max_depth)\n",
    "    temp_train_acc = []\n",
    "    temp_test_acc = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        rfc.fit(X_train, y_train)\n",
    "        temp_train_acc.append(rfc.score(X_train, y_train))\n",
    "        temp_test_acc.append(rfc.score(X_test, y_test))\n",
    "    train_acc.append(temp_train_acc)\n",
    "    test_acc.append(temp_test_acc)\n",
    "    \n",
    "train_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\n",
    "print(\"Best accuracy on CV is {:.2f}% with {} max_depth\".format(max(test_acc.mean(axis=1))*100, \n",
    "                                                        max_depth_grid[np.argmax(test_acc.mean(axis=1))]))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(max_depth_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\n",
    "ax.plot(max_depth_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='cv')\n",
    "ax.fill_between(max_depth_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\n",
    "ax.fill_between(max_depth_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylim([0.88,1.02])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Max_depth\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр `max_depth` хорошо справляется с регуляризацией модели, и мы уже не так сильно переобучаемся. Точность нашей модели немного возросла.\n",
    "\n",
    "Еще важный параметр `min_samples_leaf`, он так же выполняет функцию регуляризатора.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем списки для сохранения точности на тренировочном и тестовом датасете\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "temp_train_acc = []\n",
    "temp_test_acc = []\n",
    "min_samples_leaf_grid = [1, 3, 5, 7, 9, 11, 13, 15, 17, 20, 22, 24]\n",
    "\n",
    "# Обучаем на тренировочном датасете\n",
    "for min_samples_leaf in min_samples_leaf_grid:\n",
    "    rfc = RandomForestClassifier(n_estimators=100, max_depth=15,\n",
    "                                 random_state=42, n_jobs=-1, \n",
    "                                 oob_score=True, min_samples_leaf=min_samples_leaf)\n",
    "    temp_train_acc = []\n",
    "    temp_test_acc = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        rfc.fit(X_train, y_train)\n",
    "        temp_train_acc.append(rfc.score(X_train, y_train))\n",
    "        temp_test_acc.append(rfc.score(X_test, y_test))\n",
    "    train_acc.append(temp_train_acc)\n",
    "    test_acc.append(temp_test_acc)\n",
    "    \n",
    "train_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\n",
    "print(\"Best accuracy on CV is {:.2f}% with {} min_samples_leaf\".format(max(test_acc.mean(axis=1))*100, \n",
    "                                                        min_samples_leaf_grid[np.argmax(test_acc.mean(axis=1))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(min_samples_leaf_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\n",
    "ax.plot(min_samples_leaf_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='cv')\n",
    "ax.fill_between(min_samples_leaf_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\n",
    "ax.fill_between(min_samples_leaf_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylim([0.88,1.02])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Min_samples_leaf\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае мы не выигрываем в точности на валидации, но зато можем сильно уменьшить переобучение до 2% при сохранении точности около 90%.\n",
    "\n",
    "Рассмотрим такой параметр как `max_features`. Для задач классификации по умолчанию используется $\\large \\sqrt{n}$, где n — число признаков. Давайте проверим, оптимально ли в нашем случае использовать 4 признака или нет.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем списки для сохранения точности на тренировочном и тестовом датасете\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "temp_train_acc = []\n",
    "temp_test_acc = []\n",
    "max_features_grid = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "\n",
    "# Обучаем на тренировочном датасете\n",
    "for max_features in max_features_grid:\n",
    "    rfc = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, \n",
    "                                 oob_score=True, max_features=max_features)\n",
    "    temp_train_acc = []\n",
    "    temp_test_acc = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        rfc.fit(X_train, y_train)\n",
    "        temp_train_acc.append(rfc.score(X_train, y_train))\n",
    "        temp_test_acc.append(rfc.score(X_test, y_test))\n",
    "    train_acc.append(temp_train_acc)\n",
    "    test_acc.append(temp_test_acc)\n",
    "    \n",
    "train_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\n",
    "print(\"Best accuracy on CV is {:.2f}% with {} max_features\".format(max(test_acc.mean(axis=1))*100, \n",
    "                                                        max_features_grid[np.argmax(test_acc.mean(axis=1))]))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(max_features_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\n",
    "ax.plot(max_features_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='cv')\n",
    "ax.fill_between(max_features_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\n",
    "ax.fill_between(max_features_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylim([0.88,1.02])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Max_features\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем случае оптимальное число признаков — 4, именно с таким значением достигается наилучший результат.\n",
    "\n",
    "Мы рассмотрели, как ведут себя кривые обучения в зависимости от изменения основных параметров. Давайте теперь с помощью `GridSearch` найдем оптимальные параметры для нашего примера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделаем инициализацию параметров, по которым хотим сделать полный перебор\n",
    "parameters = {'max_features': [4, 7, 10, 13], \n",
    "              'min_samples_leaf': [1, 3, 5, 7], \n",
    "              'max_depth': [5,10,15,20]}\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42, \n",
    "                             n_jobs=-1, oob_score=True)\n",
    "gcv = GridSearchCV(rfc, parameters, n_jobs=-1, cv=skf, verbose=1)\n",
    "gcv.fit(X, y)\n",
    "\n",
    "gcv.best_estimator_, gcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Важность признаков\n",
    "rf_best = RandomForestClassifier(n_estimators=100, random_state=42, \n",
    "                             n_jobs=-1, oob_score=True,\n",
    "                                max_features = 10, max_depth=20, min_samples_leaf=3)\n",
    "rf_best.fit(X, y)\n",
    "\n",
    "pd.DataFrame(dict(zip(X.columns, rf_best.feature_importances_)), \n",
    "             index=[0]).T.sort_values(0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost classifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_wine()['data'], load_wine()['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "feature_wine = load_wine()['feature_names']\n",
    "print(pd.DataFrame(X_train, columns = feature_wine).nunique())\n",
    "print(\"Categorical features are absent in this dataset\")\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# initialize Pool\n",
    "train_pool = Pool(X_train, \n",
    "                  label = y_train, \n",
    "                  cat_features=None)\n",
    "\n",
    "test_pool = Pool(X_test, \n",
    "                 label = y_test,\n",
    "                 cat_features=None)\n",
    "\n",
    "model = CatBoostClassifier(iterations=5,\n",
    "                           depth=5,\n",
    "                           learning_rate=0.3,\n",
    "                           loss_function='MultiClass',\n",
    "                           verbose=True)\n",
    "# train the model\n",
    "model.fit(train_pool)\n",
    "# make the prediction using the resulting model\n",
    "preds_class = model.predict(test_pool)\n",
    "preds_proba = model.predict_proba(test_pool)\n",
    "print(\"class = \", preds_class[:5])\n",
    "print(\"proba = \\n\", preds_proba[:5])\n",
    "print(confusion_matrix(y_test, preds_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost regressor \n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from catboost import CatBoostRegressor\n",
    "X, y = fetch_california_housing()['data'], fetch_california_housing()['target']\n",
    "\n",
    "# initialize data (california house prices)\n",
    "feature_names = fetch_california_housing()['feature_names']\n",
    "print(f\"features are : {feature_names} \\n\")\n",
    "\n",
    "df = pd.DataFrame(X, columns = feature_names)\n",
    "print(df.nunique())\n",
    "print(\"Categorical variables are absent in this dataset\")\n",
    "\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X,y, test_size = 0.2, random_state = 17) \n",
    "\n",
    "X_labels = np.arange(0, X_train.shape[0])\n",
    "\n",
    "# initialize Pool\n",
    "train_pool_reg = Pool(X_train_reg, \n",
    "                  y_train_reg, \n",
    "                  cat_features=None)\n",
    "\n",
    "test_pool_reg = Pool(X_test_reg, \n",
    "                 cat_features=None)\n",
    "\n",
    "model = CatBoostRegressor(iterations=500, \n",
    "                          depth=6, \n",
    "                          learning_rate=0.2, \n",
    "                          loss_function='RMSE')\n",
    "#train the model\n",
    "model.fit(train_pool_reg)\n",
    "# make the prediction using the resulting model\n",
    "preds_reg = model.predict(test_pool_reg)\n",
    "print(r2_score(y_test_reg, preds_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "train_data_lgb = lightgbm.Dataset(X_train_reg, label=y_train_reg, categorical_feature=None)\n",
    "test_data_lgb = lightgbm.Dataset(X_test_reg, label=y_test_reg)\n",
    "\n",
    "parameters = {\n",
    "    'metric': 'MAE',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 50,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "model = lightgbm.train(parameters,\n",
    "                       train_data_lgb,\n",
    "                       valid_sets=test_data_lgb,\n",
    "                       num_boost_round=500,\n",
    "                       early_stopping_rounds=35)\n",
    "preds_reg_lgbm = model.predict(X_test_reg)\n",
    "print(f\"R squared on test sample = {r2_score(y_test_reg, preds_reg_lgbm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "my_model = XGBRegressor(max_depth=4, \n",
    "                        random_state=17, \n",
    "                        reg_lambda=25,\n",
    "                        reg_alpha=14,\n",
    "                        n_estimators=250,\n",
    "                        learning_rate=0.35)\n",
    "# Add silent=True to avoid printing out updates with each cycle\n",
    "my_model.fit(X_train_reg, y_train_reg, verbose=False)\n",
    "\n",
    "preds_reg_xgboost = my_model.predict(X_test_reg)\n",
    "print(f\"R squared on test sample = {r2_score(y_test_reg, preds_reg_xgboost)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация текстов при помощи эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "VECTOR_SIZE = 50 # (мы можем увеличить до 300 и более, но здесь ограничимся 50 для экономии времени обучения)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Train w2vec model\n",
    "\n",
    "KING - QUEEN = MAN - WOMAN\n",
    "![](https://i1.wp.com/www.lifestyletrading101.com/wp-content/uploads/2017/03/word2-vec-king-queen.png?resize=498%2C505)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = \"/Users/iakubovskii/Machine_Learning/Datasets/Amazon_food\"\n",
    "os.chdir(project_path)\n",
    "with open(\"unlabeled_150k.csv\", encoding=\"utf8\") as file:\n",
    "    text_df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize_text(text_df):\n",
    "    stop_words = stopwords.words('english')\n",
    "    text_list = [[x.lower() for x in nltk.word_tokenize(x) if x not in stop_words and x.isalnum()] for x in\n",
    "                 text_df['Text']]\n",
    "    return text_list\n",
    "text_df_cleaned = clean_and_tokenize_text(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(text_df_cleaned,\n",
    "                               window=50, # максимальное расстояние между текущим и предсказанным словом внутри предложения\n",
    "                               vector_size=VECTOR_SIZE,\n",
    "                               epochs=5, # количество итераций по корпусу\n",
    "                               min_count=3, # игнорирует слова, которые встречаются реже, чем 3 раза\n",
    "                               workers=3) # количество потоков\n",
    "model.save(\"model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word, model):\n",
    "    \"\"\"Трансформируем каждое слово в вектор\"\"\"\n",
    "    try:\n",
    "        return model.wv.get_vector(word)\n",
    "    except:\n",
    "        return np.zeros(VECTOR_SIZE)\n",
    "def calculate_avg_vectors(text_list, model):\n",
    "    \"\"\"Считаем средний вектор по всему документу с учетом значений векторов для каждого слова\"\"\"\n",
    "    vectors = []\n",
    "    for word in text_list:\n",
    "        vectors.append(get_vector(word, model))\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(VECTOR_SIZE)\n",
    "    vec_avg = np.mean(vectors, axis=0)\n",
    "    return vec_avg\n",
    "def create_word_embeddings(text_list, model):\n",
    "    \"\"\"Создаем датафрейм с эмбеддингами (векторами документов)\"\"\"\n",
    "    vectors = [calculate_avg_vectors(x, model) for x in text_list]\n",
    "    vectors_df = pd.DataFrame(vectors).apply(pd.Series).reset_index()\n",
    "    vectors_df.drop(\"index\", axis=1, inplace=True)\n",
    "    return vectors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "with open(\"train_40k.csv\", encoding=\"utf8\") as file:\n",
    "    text_df_train = pd.read_csv(file)\n",
    "text_df_train_cleaned = clean_and_tokenize_text(text_df_train)\n",
    "embeddings_train = create_word_embeddings(text_df_train_cleaned, model)\n",
    "target = np.where(text_df_train['Score'] > 3, 1, 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_train, target,\n",
    "                                                            stratify=target,\n",
    "                                                            test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример со схожестью отзывов (находим косинусную схожесть между векторами)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "review_vector_1 = embeddings_train.iloc[200].values\n",
    "def get_similariry(vector):\n",
    "    return cosine_similarity(review_vector_1.reshape(1,-1), vector.reshape(1,-1))\n",
    "cosine_matrix = np.apply_along_axis(get_similariry, 1, embeddings_train.values)\n",
    "pd.Series(cosine_matrix[:, 0, 0]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've used Aussie shampoos and conditioners in the past, but hadn't tried their hairspray. It holds well without being sticky and has a pleasant aroma. I priced it and that's the only thing I can find negative about this particular product. I can purchase a similar product at a lower cost.\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df_train.iloc[200]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Over the years I have tried numerous deodorant and antiperspirant products in an effort to strike a balance between reducing sweat and also not irritating my skin. Virtually no products you find in drug stores fit the bill; I have tried them. I have been using a combination of a spray-on and a hypoallergenic mineral deodorant for a few years, but this winter that combo was just not handling the sweat (and I am not an unnaturally sweaty person). I tried Pit Boss after doing a bit of research and reading customer reviews. This product is EXCELLENT and has surpassed my expectations. I can wear it daily, it has a pleasant smell but is not overpowering like many spray-on products. The only minor gripe is the tackiness on skin and hair, which you get from any antiperspirant. But after a month+ of use it has not irritated my skin once, which is more than I can say for even most non-antiperspirant deodorants I have used. At $12-15 a pop it is not cheap, but I find it well worth the extra cost. Highly, highly recommend.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df_train.iloc[28314]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I now know why this product is less expensive than that found in department stores. I have used this product for several years and know the potentcy should be stronger than that in the product I received from Amazon. This cologne is much much weaker and almost no fragrance within a few hours. I truly feel that this is watered down and will not buy again. Usually I have great satisfactiion with product purchased from Amazon but his product is inferior and not the real thing.Bob LepperSpringstead, Wi'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df_train.iloc[22704]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score = 0.7913840368019018, F_score = 0.8074137235923218\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "lgbm_w2v = LGBMClassifier(boosting_type='gbdt',\n",
    "                                    metric='binary_loglass',\n",
    "                                    max_depth=20,\n",
    "                                    n_jobs=3,\n",
    "                                    num_leaves=25,\n",
    "                                    n_estimators=80,\n",
    "                                    reg_alpha=0.3,\n",
    "                                    reg_lambda=0.5,\n",
    "                                    is_unbalance=True,\n",
    "                                    random_state=12345)\n",
    "lgbm_w2v.fit(X_train, y_train)\n",
    "predictions_w2v = lgbm_w2v.predict(X_test)\n",
    "predictions_w2v_proba = lgbm_w2v.predict_proba(X_test)[:, 1]\n",
    "auc_score = roc_auc_score(y_test, predictions_w2v_proba)\n",
    "f_score = f1_score(y_test, predictions_w2v, average='weighted')\n",
    "\n",
    "print(f\"ROC-AUC score = {auc_score}, F_score = {f_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Predict new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"val_10k.csv\", encoding=\"utf8\") as file:\n",
    "    text_df_validated = pd.read_csv(file)\n",
    "    \n",
    "text_df_validated_cleaned = clean_and_tokenize_text(text_df_validated)\n",
    "embeddings_validate = create_word_embeddings(text_df_validated_cleaned, model)\n",
    "y_validated = np.where(text_df_validated['Score'] > 3, 1, 0)\n",
    "\n",
    "predictions_w2v_validated = lgbm_w2v.predict(embeddings_validate)\n",
    "predictions_w2v_validated_proba = lgbm_w2v.predict_proba(embeddings_validate)[:, 1]\n",
    "auc_score_validated = roc_auc_score(y_validated, predictions_w2v_validated)\n",
    "f_score_validated = f1_score(y_validated, predictions_w2v_validated, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score validated = 0.7832728291979412, F_score validated = 0.8109887859697004\n"
     ]
    }
   ],
   "source": [
    "print(f\"ROC-AUC score validated = {auc_score_validated}, F_score validated = {f_score_validated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Байесовская оптимизация гиперпараметров "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Searching for the next optimal point.\n",
      "Iteration No: 1 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.9641\n",
      "Function value obtained: -0.8425\n",
      "Current minimum: -0.8425\n",
      "Iteration No: 2 started. Searching for the next optimal point.\n",
      "Iteration No: 2 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.7094\n",
      "Function value obtained: -0.8389\n",
      "Current minimum: -0.8425\n",
      "Iteration No: 3 started. Searching for the next optimal point.\n",
      "Iteration No: 3 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.1598\n",
      "Function value obtained: -0.8431\n",
      "Current minimum: -0.8431\n",
      "Iteration No: 4 started. Searching for the next optimal point.\n",
      "Iteration No: 4 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.7632\n",
      "Function value obtained: -0.8399\n",
      "Current minimum: -0.8431\n",
      "Iteration No: 5 started. Searching for the next optimal point.\n",
      "Iteration No: 5 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.8462\n",
      "Function value obtained: -0.8440\n",
      "Current minimum: -0.8440\n",
      "Iteration No: 6 started. Searching for the next optimal point.\n",
      "Iteration No: 6 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.8653\n",
      "Function value obtained: -0.8360\n",
      "Current minimum: -0.8440\n",
      "Iteration No: 7 started. Searching for the next optimal point.\n",
      "Iteration No: 7 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.3754\n",
      "Function value obtained: -0.8420\n",
      "Current minimum: -0.8440\n",
      "Iteration No: 8 started. Searching for the next optimal point.\n",
      "Iteration No: 8 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.5789\n",
      "Function value obtained: -0.8474\n",
      "Current minimum: -0.8474\n",
      "Iteration No: 9 started. Searching for the next optimal point.\n",
      "Iteration No: 9 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.6859\n",
      "Function value obtained: -0.8379\n",
      "Current minimum: -0.8474\n",
      "Iteration No: 10 started. Searching for the next optimal point.\n",
      "Iteration No: 10 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.0826\n",
      "Function value obtained: -0.8370\n",
      "Current minimum: -0.8474\n",
      "Iteration No: 11 started. Searching for the next optimal point.\n",
      "Iteration No: 11 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.6439\n",
      "Function value obtained: -0.8427\n",
      "Current minimum: -0.8474\n",
      "Iteration No: 12 started. Searching for the next optimal point.\n",
      "Iteration No: 12 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.8354\n",
      "Function value obtained: -0.8426\n",
      "Current minimum: -0.8474\n",
      "Iteration No: 13 started. Searching for the next optimal point.\n",
      "Iteration No: 13 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.9571\n",
      "Function value obtained: -0.8339\n",
      "Current minimum: -0.8474\n",
      "Iteration No: 14 started. Searching for the next optimal point.\n",
      "Iteration No: 14 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.4043\n",
      "Function value obtained: -0.8527\n",
      "Current minimum: -0.8527\n",
      "Iteration No: 15 started. Searching for the next optimal point.\n",
      "Iteration No: 15 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.7172\n",
      "Function value obtained: -0.8397\n",
      "Current minimum: -0.8527\n",
      "Iteration No: 16 started. Searching for the next optimal point.\n",
      "Iteration No: 16 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.1219\n",
      "Function value obtained: -0.8452\n",
      "Current minimum: -0.8527\n",
      "Iteration No: 17 started. Searching for the next optimal point.\n",
      "Iteration No: 17 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.9913\n",
      "Function value obtained: -0.8452\n",
      "Current minimum: -0.8527\n",
      "Iteration No: 18 started. Searching for the next optimal point.\n",
      "Iteration No: 18 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.1279\n",
      "Function value obtained: -0.8365\n",
      "Current minimum: -0.8527\n",
      "Iteration No: 19 started. Searching for the next optimal point.\n",
      "Iteration No: 19 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.3026\n",
      "Function value obtained: -0.8414\n",
      "Current minimum: -0.8527\n",
      "Iteration No: 20 started. Searching for the next optimal point.\n",
      "Iteration No: 20 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.1995\n",
      "Function value obtained: -0.8460\n",
      "Current minimum: -0.8527\n",
      "Iteration No: 21 started. Searching for the next optimal point.\n",
      "Iteration No: 21 ended. Search finished for the next optimal point.\n",
      "Time taken: 3.1585\n",
      "Function value obtained: -0.8475\n",
      "Current minimum: -0.8527\n",
      "Iteration No: 22 started. Searching for the next optimal point.\n",
      "Iteration No: 22 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.4990\n",
      "Function value obtained: -0.8456\n",
      "Current minimum: -0.8527\n",
      "Iteration No: 23 started. Searching for the next optimal point.\n",
      "Iteration No: 23 ended. Search finished for the next optimal point.\n",
      "Time taken: 3.0547\n",
      "Function value obtained: -0.8414\n",
      "Current minimum: -0.8527\n",
      "Iteration No: 24 started. Searching for the next optimal point.\n",
      "Iteration No: 24 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.1446\n",
      "Function value obtained: -0.8511\n",
      "Current minimum: -0.8527\n",
      "Iteration No: 25 started. Searching for the next optimal point.\n",
      "Iteration No: 25 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.0199\n",
      "Function value obtained: -0.8338\n",
      "Current minimum: -0.8527\n",
      "Iteration No: 26 started. Searching for the next optimal point.\n",
      "Iteration No: 26 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.9578\n",
      "Function value obtained: -0.8449\n",
      "Current minimum: -0.8527\n",
      "Iteration No: 27 started. Searching for the next optimal point.\n",
      "Iteration No: 27 ended. Search finished for the next optimal point.\n",
      "Time taken: 3.2957\n",
      "Function value obtained: -0.8420\n",
      "Current minimum: -0.8527\n",
      "Iteration No: 28 started. Searching for the next optimal point.\n",
      "Iteration No: 28 ended. Search finished for the next optimal point.\n",
      "Time taken: 3.3864\n",
      "Function value obtained: -0.8442\n",
      "Current minimum: -0.8527\n",
      "Iteration No: 29 started. Searching for the next optimal point.\n",
      "Iteration No: 29 ended. Search finished for the next optimal point.\n",
      "Time taken: 3.5702\n",
      "Function value obtained: -0.8438\n",
      "Current minimum: -0.8527\n",
      "Iteration No: 30 started. Searching for the next optimal point.\n",
      "Iteration No: 30 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.8054\n",
      "Function value obtained: -0.8390\n",
      "Current minimum: -0.8527\n",
      "Iteration No: 31 started. Searching for the next optimal point.\n",
      "LGBM took 59.80 seconds,  candidates checked: 30, best CV score: 0.853 ± 0.020\n",
      "Best parameters:\n",
      "OrderedDict([('learning_rate', 0.2406383605989789),\n",
      "             ('max_depth', 22),\n",
      "             ('n_estimators', 200),\n",
      "             ('num_leaves', 58)])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import pprint\n",
    "from sklearn.metrics import make_scorer\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.callbacks import DeadlineStopper, VerboseCallback\n",
    "from skopt.space import Real, Integer\n",
    "roc_auc_opt = make_scorer(roc_auc_score, greater_is_better=True, needs_threshold=True)\n",
    "\n",
    "# Defining your search space\n",
    "search_spaces = {\"num_leaves\": Integer(10,100),\n",
    "                 'max_depth': Integer(5, 30),\n",
    "                 'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "                 'n_estimators': Integer(100, 200)\n",
    "                 }\n",
    "lgbm_clf = LGBMClassifier(boosting_type = \"gbdt\", objective='binary', n_jobs=-1)\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 17)\n",
    "# Setting up BayesSearchCV\n",
    "opt = BayesSearchCV(lgbm_clf,\n",
    "                    search_spaces,\n",
    "                    scoring=roc_auc_opt,\n",
    "                    cv=skf,\n",
    "                    n_iter=30,\n",
    "                    n_jobs=-1,  # use just 1 job with CatBoost in order to avoid segmentation fault\n",
    "                    return_train_score=False,\n",
    "                    refit=True,\n",
    "                    random_state=42)\n",
    "# Reporting util for different optimizers\n",
    "def report_perf(optimizer, X, y, title, callbacks=None):\n",
    "    \"\"\"\n",
    "    Функция для оценки времени и параметров оптимизации\n",
    "    \n",
    "    optimizer = skopt объект\n",
    "    X = признаки\n",
    "    y = целевая переменная\n",
    "    title = название эксперимента\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    if callbacks:\n",
    "        optimizer.fit(X, y, callback=callbacks)\n",
    "    else:\n",
    "        optimizer.fit(X, y)\n",
    "    d=pd.DataFrame(optimizer.cv_results_)\n",
    "    best_score = optimizer.best_score_\n",
    "    best_score_std = d.iloc[optimizer.best_index_].std_test_score\n",
    "    best_params = optimizer.best_params_\n",
    "    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n",
    "           +u\"\\u00B1\"+\" %.3f\") % (time() - start, \n",
    "                                  len(optimizer.cv_results_['params']),\n",
    "                                  best_score,\n",
    "                                  best_score_std))    \n",
    "    print('Best parameters:')\n",
    "    pprint.pprint(best_params)\n",
    "    print()\n",
    "    return best_params\n",
    "\n",
    "best_params = report_perf(opt, X_train, y_train,'LGBM', \n",
    "                          callbacks=[VerboseCallback(100), \n",
    "                                     DeadlineStopper(60*10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('learning_rate', 0.2406383605989789),\n",
       "             ('max_depth', 22),\n",
       "             ('n_estimators', 200),\n",
       "             ('num_leaves', 58)])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC score validated = 0.7269914961219484, F_score validated = 0.8392017153649144\n"
     ]
    }
   ],
   "source": [
    "lgbm_best_params = LGBMClassifier(learning_rate=best_params['learning_rate'],\n",
    "                                  max_depth=best_params['max_depth'],\n",
    "                                  n_estimators=best_params['n_estimators'],\n",
    "                                 num_leaves=best_params['num_leaves']).fit(X_train, y_train)\n",
    "predictions_w2v_validated = lgbm_best_params.predict(embeddings_validate)\n",
    "predictions_w2v_validated_proba = lgbm_best_params.predict_proba(embeddings_validate)[:, 1]\n",
    "auc_score_validated = roc_auc_score(y_validated, predictions_w2v_validated_proba)\n",
    "f_score_validated = f1_score(y_validated, predictions_w2v_validated, average='weighted')\n",
    "print(f\"ROC-AUC score validated = {auc_score_validated}, F_score validated = {f_score_validated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Возможна ошибка из-за несовместимости пакета scikit-optimize c новой версией sklearn 0.24. Чтобы успешно юзать\n",
    "байесовскую оптимизацию, нам нужно откатить sklearn до версии 0.23.4. Чтобы откатиться на старую версию, в терминале введите следующее:\n",
    "\n",
    "pip install scikit-learn==0.23.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.1'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}